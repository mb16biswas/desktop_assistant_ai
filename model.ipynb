{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "comprehensive-pointer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "defined-crash",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the dataset \n",
    "import json \n",
    "with open(\"Data.json\") as file :\n",
    "    data = json.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "patent-workshop",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'intents': [{'tag': 'google',\n",
       "   'code': 0,\n",
       "   'patterns': ['can you search this in google',\n",
       "    'open google',\n",
       "    'google search',\n",
       "    'search something in google',\n",
       "    'find this in google ',\n",
       "    'search this in google',\n",
       "    'look in google',\n",
       "    'find this in google',\n",
       "    'google',\n",
       "    'search google',\n",
       "    'find in google']},\n",
       "  {'tag': 'youtube',\n",
       "   'code': 1,\n",
       "   'patterns': ['can you search this in youtube',\n",
       "    'open youtube',\n",
       "    'youtube search',\n",
       "    'search something in youtube',\n",
       "    'find this in youtube',\n",
       "    'search this in youtube',\n",
       "    'look in youtube',\n",
       "    'find this in youtube',\n",
       "    'youtube',\n",
       "    'search youtube',\n",
       "    'find in youtube']},\n",
       "  {'tag': 'email',\n",
       "   'code': 2,\n",
       "   'patterns': ['send email',\n",
       "    'send email to mike',\n",
       "    'email',\n",
       "    'email send',\n",
       "    'email to him',\n",
       "    'email him',\n",
       "    'send a email',\n",
       "    'email send',\n",
       "    'can u please send a email',\n",
       "    'send email to jenny',\n",
       "    'can u send the email',\n",
       "    'send the email']},\n",
       "  {'tag': 'random',\n",
       "   'code': 3,\n",
       "   'patterns': ['What hours are you open?',\n",
       "    'What are your hours?',\n",
       "    'When are you open?',\n",
       "    'Hi',\n",
       "    'How are you',\n",
       "    'Is anyone there?',\n",
       "    'Hello',\n",
       "    'Good day',\n",
       "    'Bye',\n",
       "    'See you later',\n",
       "    'Goodbye',\n",
       "    'good day',\n",
       "    'lalalalalalalalal',\n",
       "    'hola']},\n",
       "  {'tag': 'text',\n",
       "   'code': 4,\n",
       "   'patterns': ['send a text message',\n",
       "    'send a text message',\n",
       "    'message',\n",
       "    'send a text message to someone',\n",
       "    'message him',\n",
       "    'send a message',\n",
       "    'send a text message',\n",
       "    'message send',\n",
       "    'can u please send a text message',\n",
       "    'message',\n",
       "    'send message',\n",
       "    'can u send the message']}]}"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "premium-dallas",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "innocent-atlantic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "judicial-wireless",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = []\n",
    "store = df[\"intents\"]\n",
    "for i in range(len(store)) : \n",
    "  PAT = store[i][\"patterns\"]\n",
    "  for j in range(len(PAT)):\n",
    "    pattern.append(PAT[j].lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "restricted-sperm",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['can you search this in google',\n",
       " 'open google',\n",
       " 'google search',\n",
       " 'search something in google',\n",
       " 'find this in google ',\n",
       " 'search this in google',\n",
       " 'look in google',\n",
       " 'find this in google',\n",
       " 'google',\n",
       " 'search google',\n",
       " 'find in google',\n",
       " 'can you search this in youtube',\n",
       " 'open youtube',\n",
       " 'youtube search',\n",
       " 'search something in youtube',\n",
       " 'find this in youtube',\n",
       " 'search this in youtube',\n",
       " 'look in youtube',\n",
       " 'find this in youtube',\n",
       " 'youtube',\n",
       " 'search youtube',\n",
       " 'find in youtube',\n",
       " 'send email',\n",
       " 'send email to mike',\n",
       " 'email',\n",
       " 'email send',\n",
       " 'email to him',\n",
       " 'email him',\n",
       " 'send a email',\n",
       " 'email send',\n",
       " 'can u please send a email',\n",
       " 'send email to jenny',\n",
       " 'can u send the email',\n",
       " 'send the email',\n",
       " 'what hours are you open?',\n",
       " 'what are your hours?',\n",
       " 'when are you open?',\n",
       " 'hi',\n",
       " 'how are you',\n",
       " 'is anyone there?',\n",
       " 'hello',\n",
       " 'good day',\n",
       " 'bye',\n",
       " 'see you later',\n",
       " 'goodbye',\n",
       " 'good day',\n",
       " 'lalalalalalalalal',\n",
       " 'hola',\n",
       " 'send a text message',\n",
       " 'send a text message',\n",
       " 'message',\n",
       " 'send a text message to someone',\n",
       " 'message him',\n",
       " 'send a message',\n",
       " 'send a text message',\n",
       " 'message send',\n",
       " 'can u please send a text message',\n",
       " 'message',\n",
       " 'send message',\n",
       " 'can u send the message']"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weekly-bidding",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "here are two really important things here - firstly our misspelt and rare words are just gone. That's really bad, we're trying to judge if a sentence is sincere and part of Quora's critera is that the sentence is gramatically correct - we've just broken that. There is also information in the fact that the word was uncommon enough to not be in the tokenizer.\n",
    "\n",
    "Another issue is the tokenizer has stripped , and ?. We might not care so much about ,s but part of the critera for a sincere question is it is in fact a question, a ? undoubtably helps us here.\n",
    "\n",
    "Second attempt - \n",
    "use an OOV token\n",
    "Keras lets us define an Out Of Vocab token - this will replace any unknown words with a token of our choosing. This is better than just throwing away unknown words since it tells our model there was information here.\n",
    "\n",
    "Let's do that\n",
    "\n",
    "Third attempt - use question marks \n",
    "\n",
    "Finally, let's fix the ? issue. The ? is being filtered out by the tokenizer, we can solve this by specifying the filters ourselves\n",
    "\n",
    "tokenizer_3 = Tokenizer(num_words=max_features, oov_token='OOV', filters='!\"#$%&()*+,-./:;<=>@[\\]^_`{|}~ ')\n",
    "tokenizer_3.fit_on_texts(list(df['question_text'].values))\n",
    "\n",
    "\n",
    "ref - https://www.kaggle.com/hamishdickson/using-keras-oov-tokens\n",
    "ref - https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "armed-pontiac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'OOV': 1, 'send': 2, 'in': 3, 'email': 4, 'message': 5, 'google': 6, 'youtube': 7, 'search': 8, 'this': 9, 'a': 10, 'can': 11, 'you': 12, 'find': 13, 'text': 14, 'to': 15, 'u': 16, 'are': 17, 'him': 18, 'the': 19, 'open': 20, 'something': 21, 'look': 22, 'please': 23, 'what': 24, 'open?': 25, 'good': 26, 'day': 27, 'mike': 28, 'jenny': 29, 'hours': 30, 'your': 31, 'hours?': 32, 'when': 33, 'hi': 34, 'how': 35, 'is': 36, 'anyone': 37, 'there?': 38, 'hello': 39, 'bye': 40, 'see': 41, 'later': 42, 'goodbye': 43, 'lalalalalalalalal': 44, 'hola': 45, 'someone': 46}\n",
      "47\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(oov_token='OOV', filters='!\"#$%&()*+,-./:;<=>@[\\]^_`{|}~ ')\n",
    "corpus =  pattern \n",
    "tokenizer.fit_on_texts(corpus)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "print(tokenizer.word_index)\n",
    "print(total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "flying-dispatch",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_list = tokenizer.texts_to_sequences([corpus[2]])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "simplified-bouquet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 8]"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "tropical-cyprus",
   "metadata": {},
   "outputs": [],
   "source": [
    "encode = []\n",
    "for i in corpus : \n",
    "\n",
    "  token = tokenizer.texts_to_sequences([i])[0]\n",
    "  encode.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "integral-chicago",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[11, 12, 8, 9, 3, 6],\n",
       " [20, 6],\n",
       " [6, 8],\n",
       " [8, 21, 3, 6],\n",
       " [13, 9, 3, 6],\n",
       " [8, 9, 3, 6],\n",
       " [22, 3, 6],\n",
       " [13, 9, 3, 6],\n",
       " [6],\n",
       " [8, 6],\n",
       " [13, 3, 6],\n",
       " [11, 12, 8, 9, 3, 7],\n",
       " [20, 7],\n",
       " [7, 8],\n",
       " [8, 21, 3, 7],\n",
       " [13, 9, 3, 7],\n",
       " [8, 9, 3, 7],\n",
       " [22, 3, 7],\n",
       " [13, 9, 3, 7],\n",
       " [7],\n",
       " [8, 7],\n",
       " [13, 3, 7],\n",
       " [2, 4],\n",
       " [2, 4, 15, 28],\n",
       " [4],\n",
       " [4, 2],\n",
       " [4, 15, 18],\n",
       " [4, 18],\n",
       " [2, 10, 4],\n",
       " [4, 2],\n",
       " [11, 16, 23, 2, 10, 4],\n",
       " [2, 4, 15, 29],\n",
       " [11, 16, 2, 19, 4],\n",
       " [2, 19, 4],\n",
       " [24, 30, 17, 12, 25],\n",
       " [24, 17, 31, 32],\n",
       " [33, 17, 12, 25],\n",
       " [34],\n",
       " [35, 17, 12],\n",
       " [36, 37, 38],\n",
       " [39],\n",
       " [26, 27],\n",
       " [40],\n",
       " [41, 12, 42],\n",
       " [43],\n",
       " [26, 27],\n",
       " [44],\n",
       " [45],\n",
       " [2, 10, 14, 5],\n",
       " [2, 10, 14, 5],\n",
       " [5],\n",
       " [2, 10, 14, 5, 15, 46],\n",
       " [5, 18],\n",
       " [2, 10, 5],\n",
       " [2, 10, 14, 5],\n",
       " [5, 2],\n",
       " [11, 16, 23, 2, 10, 14, 5],\n",
       " [5],\n",
       " [2, 5],\n",
       " [11, 16, 2, 19, 5]]"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "subjective-republican",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_len = max([len(x) for x in encode])\n",
    "input_sequences_X  = np.array(pad_sequences(encode , maxlen=max_sequence_len, padding='pre'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "national-mentor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0, 11, 12,  8,  9,  3,  6],\n",
       "       [ 0,  0,  0,  0,  0, 20,  6],\n",
       "       [ 0,  0,  0,  0,  0,  6,  8],\n",
       "       [ 0,  0,  0,  8, 21,  3,  6],\n",
       "       [ 0,  0,  0, 13,  9,  3,  6],\n",
       "       [ 0,  0,  0,  8,  9,  3,  6],\n",
       "       [ 0,  0,  0,  0, 22,  3,  6],\n",
       "       [ 0,  0,  0, 13,  9,  3,  6],\n",
       "       [ 0,  0,  0,  0,  0,  0,  6],\n",
       "       [ 0,  0,  0,  0,  0,  8,  6],\n",
       "       [ 0,  0,  0,  0, 13,  3,  6],\n",
       "       [ 0, 11, 12,  8,  9,  3,  7],\n",
       "       [ 0,  0,  0,  0,  0, 20,  7],\n",
       "       [ 0,  0,  0,  0,  0,  7,  8],\n",
       "       [ 0,  0,  0,  8, 21,  3,  7],\n",
       "       [ 0,  0,  0, 13,  9,  3,  7],\n",
       "       [ 0,  0,  0,  8,  9,  3,  7],\n",
       "       [ 0,  0,  0,  0, 22,  3,  7],\n",
       "       [ 0,  0,  0, 13,  9,  3,  7],\n",
       "       [ 0,  0,  0,  0,  0,  0,  7],\n",
       "       [ 0,  0,  0,  0,  0,  8,  7],\n",
       "       [ 0,  0,  0,  0, 13,  3,  7],\n",
       "       [ 0,  0,  0,  0,  0,  2,  4],\n",
       "       [ 0,  0,  0,  2,  4, 15, 28],\n",
       "       [ 0,  0,  0,  0,  0,  0,  4],\n",
       "       [ 0,  0,  0,  0,  0,  4,  2],\n",
       "       [ 0,  0,  0,  0,  4, 15, 18],\n",
       "       [ 0,  0,  0,  0,  0,  4, 18],\n",
       "       [ 0,  0,  0,  0,  2, 10,  4],\n",
       "       [ 0,  0,  0,  0,  0,  4,  2],\n",
       "       [ 0, 11, 16, 23,  2, 10,  4],\n",
       "       [ 0,  0,  0,  2,  4, 15, 29],\n",
       "       [ 0,  0, 11, 16,  2, 19,  4],\n",
       "       [ 0,  0,  0,  0,  2, 19,  4],\n",
       "       [ 0,  0, 24, 30, 17, 12, 25],\n",
       "       [ 0,  0,  0, 24, 17, 31, 32],\n",
       "       [ 0,  0,  0, 33, 17, 12, 25],\n",
       "       [ 0,  0,  0,  0,  0,  0, 34],\n",
       "       [ 0,  0,  0,  0, 35, 17, 12],\n",
       "       [ 0,  0,  0,  0, 36, 37, 38],\n",
       "       [ 0,  0,  0,  0,  0,  0, 39],\n",
       "       [ 0,  0,  0,  0,  0, 26, 27],\n",
       "       [ 0,  0,  0,  0,  0,  0, 40],\n",
       "       [ 0,  0,  0,  0, 41, 12, 42],\n",
       "       [ 0,  0,  0,  0,  0,  0, 43],\n",
       "       [ 0,  0,  0,  0,  0, 26, 27],\n",
       "       [ 0,  0,  0,  0,  0,  0, 44],\n",
       "       [ 0,  0,  0,  0,  0,  0, 45],\n",
       "       [ 0,  0,  0,  2, 10, 14,  5],\n",
       "       [ 0,  0,  0,  2, 10, 14,  5],\n",
       "       [ 0,  0,  0,  0,  0,  0,  5],\n",
       "       [ 0,  2, 10, 14,  5, 15, 46],\n",
       "       [ 0,  0,  0,  0,  0,  5, 18],\n",
       "       [ 0,  0,  0,  0,  2, 10,  5],\n",
       "       [ 0,  0,  0,  2, 10, 14,  5],\n",
       "       [ 0,  0,  0,  0,  0,  5,  2],\n",
       "       [11, 16, 23,  2, 10, 14,  5],\n",
       "       [ 0,  0,  0,  0,  0,  0,  5],\n",
       "       [ 0,  0,  0,  0,  0,  2,  5],\n",
       "       [ 0,  0, 11, 16,  2, 19,  5]])"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sequences_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "forced-nickname",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'intents': [{'tag': 'google',\n",
       "   'code': 0,\n",
       "   'patterns': ['can you search this in google',\n",
       "    'open google',\n",
       "    'google search',\n",
       "    'search something in google',\n",
       "    'find this in google ',\n",
       "    'search this in google',\n",
       "    'look in google',\n",
       "    'find this in google',\n",
       "    'google',\n",
       "    'search google',\n",
       "    'find in google']},\n",
       "  {'tag': 'youtube',\n",
       "   'code': 1,\n",
       "   'patterns': ['can you search this in youtube',\n",
       "    'open youtube',\n",
       "    'youtube search',\n",
       "    'search something in youtube',\n",
       "    'find this in youtube',\n",
       "    'search this in youtube',\n",
       "    'look in youtube',\n",
       "    'find this in youtube',\n",
       "    'youtube',\n",
       "    'search youtube',\n",
       "    'find in youtube']},\n",
       "  {'tag': 'email',\n",
       "   'code': 2,\n",
       "   'patterns': ['send email',\n",
       "    'send email to mike',\n",
       "    'email',\n",
       "    'email send',\n",
       "    'email to him',\n",
       "    'email him',\n",
       "    'send a email',\n",
       "    'email send',\n",
       "    'can u please send a email',\n",
       "    'send email to jenny',\n",
       "    'can u send the email',\n",
       "    'send the email']},\n",
       "  {'tag': 'random',\n",
       "   'code': 3,\n",
       "   'patterns': ['What hours are you open?',\n",
       "    'What are your hours?',\n",
       "    'When are you open?',\n",
       "    'Hi',\n",
       "    'How are you',\n",
       "    'Is anyone there?',\n",
       "    'Hello',\n",
       "    'Good day',\n",
       "    'Bye',\n",
       "    'See you later',\n",
       "    'Goodbye',\n",
       "    'good day',\n",
       "    'lalalalalalalalal',\n",
       "    'hola']},\n",
       "  {'tag': 'text',\n",
       "   'code': 4,\n",
       "   'patterns': ['send a text message',\n",
       "    'send a text message',\n",
       "    'message',\n",
       "    'send a text message to someone',\n",
       "    'message him',\n",
       "    'send a message',\n",
       "    'send a text message',\n",
       "    'message send',\n",
       "    'can u please send a text message',\n",
       "    'message',\n",
       "    'send message',\n",
       "    'can u send the message']}]}"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "annoying-reception",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = []\n",
    "for i in df[\"intents\"]:\n",
    "    a = [i[\"code\"]]*len(i[\"patterns\"])\n",
    "    for j in a : \n",
    "        pred_y.append(j)\n",
    "\n",
    "        \n",
    "# [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "# [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "# [2, 2, 2, 2, 2]\n",
    "# [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
    "# [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "expired-literacy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n"
     ]
    }
   ],
   "source": [
    "#catogorical \n",
    "print(pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "cosmetic-chick",
   "metadata": {},
   "outputs": [],
   "source": [
    " pred_y_cat = tf.keras.utils.to_categorical(\n",
    "    pred_y, num_classes = 5 , dtype='float32'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "creative-imperial",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "   input_sequences_X, pred_y, test_size=0.1, random_state=42)\n",
    "    \n",
    "    \n",
    "X_train_cat, X_test_cat, y_train_cat, y_test_cat = train_test_split(\n",
    "   input_sequences_X, pred_y_cat, test_size=0.1, random_state=42)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "established-black",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  0  2 19  4]\n",
      " [ 0  0  0  2 10 14  5]\n",
      " [ 0  0  0  0  0 20  7]\n",
      " [ 0  0  0  0  0  0  5]\n",
      " [ 0  0  0  0  0  0 44]\n",
      " [ 0  0  0  0  0  0  5]\n",
      " [ 0  0  0  2  4 15 29]\n",
      " [ 0  0  0  8 21  3  6]\n",
      " [ 0  0  0  0  0  5 18]\n",
      " [ 0  0  0  0 22  3  7]\n",
      " [ 0  0  0  0  0  0  6]\n",
      " [ 0  0  0  0 22  3  6]\n",
      " [ 0  0  0  0  0  0 39]\n",
      " [ 0  0  0 13  9  3  6]\n",
      " [ 0  0  0  0 41 12 42]\n",
      " [ 0  0  0  0  0  0  7]\n",
      " [ 0  0 24 30 17 12 25]\n",
      " [ 0  0  0  0  0  2  5]\n",
      " [ 0  0  0  0  0  4  2]\n",
      " [11 16 23  2 10 14  5]\n",
      " [ 0  0  0 13  9  3  7]\n",
      " [ 0  0  0  0  0  4 18]\n",
      " [ 0  0  0  0  0  8  6]\n",
      " [ 0 11 16 23  2 10  4]\n",
      " [ 0  0  0  0  4 15 18]\n",
      " [ 0  0  0  8  9  3  7]\n",
      " [ 0  0  0  0  0  0  4]\n",
      " [ 0  0  0  0  0  5  2]\n",
      " [ 0 11 12  8  9  3  7]\n",
      " [ 0  0 11 16  2 19  4]\n",
      " [ 0  0  0  0  2 10  5]\n",
      " [ 0  0  0  0  0 26 27]\n",
      " [ 0  0  0  0  0  0 34]\n",
      " [ 0  0  0  0  0  4  2]\n",
      " [ 0  0  0  0  0  0 43]\n",
      " [ 0  0  0  0  0 20  6]\n",
      " [ 0  0  0  0 13  3  7]\n",
      " [ 0  0  0  0  0  6  8]\n",
      " [ 0  0  0  0  0  0 45]\n",
      " [ 0  0  0  0 36 37 38]\n",
      " [ 0  0  0 24 17 31 32]\n",
      " [ 0  0  0  2  4 15 28]\n",
      " [ 0  0  0  2 10 14  5]\n",
      " [ 0  0  0  0 13  3  6]\n",
      " [ 0  0  0  0  0  2  4]\n",
      " [ 0  0  0 13  9  3  7]\n",
      " [ 0  0 11 16  2 19  5]\n",
      " [ 0  0  0  0  0  8  7]\n",
      " [ 0  0  0 13  9  3  6]\n",
      " [ 0  0  0  0  0  0 40]\n",
      " [ 0  0  0  8 21  3  7]\n",
      " [ 0  0  0  0  2 10  4]\n",
      " [ 0  2 10 14  5 15 46]\n",
      " [ 0  0  0  0 35 17 12]] [2, 4, 1, 4, 3, 4, 2, 0, 4, 1, 0, 0, 3, 0, 3, 1, 3, 4, 2, 4, 1, 2, 0, 2, 2, 1, 2, 4, 1, 2, 4, 3, 3, 2, 3, 0, 1, 0, 3, 3, 3, 2, 4, 0, 2, 1, 4, 1, 0, 3, 1, 2, 4, 3]\n",
      "[[ 0  0  0  0  2 19  4]\n",
      " [ 0  0  0  2 10 14  5]\n",
      " [ 0  0  0  0  0 20  7]\n",
      " [ 0  0  0  0  0  0  5]\n",
      " [ 0  0  0  0  0  0 44]\n",
      " [ 0  0  0  0  0  0  5]\n",
      " [ 0  0  0  2  4 15 29]\n",
      " [ 0  0  0  8 21  3  6]\n",
      " [ 0  0  0  0  0  5 18]\n",
      " [ 0  0  0  0 22  3  7]\n",
      " [ 0  0  0  0  0  0  6]\n",
      " [ 0  0  0  0 22  3  6]\n",
      " [ 0  0  0  0  0  0 39]\n",
      " [ 0  0  0 13  9  3  6]\n",
      " [ 0  0  0  0 41 12 42]\n",
      " [ 0  0  0  0  0  0  7]\n",
      " [ 0  0 24 30 17 12 25]\n",
      " [ 0  0  0  0  0  2  5]\n",
      " [ 0  0  0  0  0  4  2]\n",
      " [11 16 23  2 10 14  5]\n",
      " [ 0  0  0 13  9  3  7]\n",
      " [ 0  0  0  0  0  4 18]\n",
      " [ 0  0  0  0  0  8  6]\n",
      " [ 0 11 16 23  2 10  4]\n",
      " [ 0  0  0  0  4 15 18]\n",
      " [ 0  0  0  8  9  3  7]\n",
      " [ 0  0  0  0  0  0  4]\n",
      " [ 0  0  0  0  0  5  2]\n",
      " [ 0 11 12  8  9  3  7]\n",
      " [ 0  0 11 16  2 19  4]\n",
      " [ 0  0  0  0  2 10  5]\n",
      " [ 0  0  0  0  0 26 27]\n",
      " [ 0  0  0  0  0  0 34]\n",
      " [ 0  0  0  0  0  4  2]\n",
      " [ 0  0  0  0  0  0 43]\n",
      " [ 0  0  0  0  0 20  6]\n",
      " [ 0  0  0  0 13  3  7]\n",
      " [ 0  0  0  0  0  6  8]\n",
      " [ 0  0  0  0  0  0 45]\n",
      " [ 0  0  0  0 36 37 38]\n",
      " [ 0  0  0 24 17 31 32]\n",
      " [ 0  0  0  2  4 15 28]\n",
      " [ 0  0  0  2 10 14  5]\n",
      " [ 0  0  0  0 13  3  6]\n",
      " [ 0  0  0  0  0  2  4]\n",
      " [ 0  0  0 13  9  3  7]\n",
      " [ 0  0 11 16  2 19  5]\n",
      " [ 0  0  0  0  0  8  7]\n",
      " [ 0  0  0 13  9  3  6]\n",
      " [ 0  0  0  0  0  0 40]\n",
      " [ 0  0  0  8 21  3  7]\n",
      " [ 0  0  0  0  2 10  4]\n",
      " [ 0  2 10 14  5 15 46]\n",
      " [ 0  0  0  0 35 17 12]] [[0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train , y_train)\n",
    "print(X_train_cat , y_train_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "bizarre-appreciation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model1\n",
    "model1 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(total_words, 32 , input_length= max_sequence_len ),\n",
    "    tf.keras.layers.LSTM(32),\n",
    "    tf.keras.layers.Dense(5, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "satellite-silver",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.compile(loss=\"categorical_crossentropy\",optimizer=\"rmsprop\",metrics=['acc'])\n",
    "#for encoded "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "frank-interaction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 7, 32)             1504      \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 165       \n",
      "=================================================================\n",
      "Total params: 9,989\n",
      "Trainable params: 9,989\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "listed-stone",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2/2 [==============================] - 0s 213ms/step - loss: 1.6104 - acc: 0.0926 - val_loss: 1.6100 - val_acc: 0.3333\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 1.6002 - acc: 0.4815 - val_loss: 1.6036 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 1.5927 - acc: 0.4259 - val_loss: 1.5984 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 1.5852 - acc: 0.5185 - val_loss: 1.5926 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 1.5772 - acc: 0.5000 - val_loss: 1.5857 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 1.5696 - acc: 0.5185 - val_loss: 1.5765 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 1.5609 - acc: 0.5185 - val_loss: 1.5671 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 1.5519 - acc: 0.5000 - val_loss: 1.5559 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 1.5413 - acc: 0.5185 - val_loss: 1.5433 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 1.5314 - acc: 0.4815 - val_loss: 1.5318 - val_acc: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fff1fe9a0>"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.fit(X_train_cat , y_train_cat , epochs = 10 ,  validation_data=(X_test_cat, y_test_cat ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "geological-radiation",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.compile(loss=\"sparse_categorical_crossentropy\",optimizer=\"rmsprop\",metrics=['acc'])\n",
    "#not encoded "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "insured-mount",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "polished-stranger",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2/2 [==============================] - 0s 215ms/step - loss: 1.5168 - acc: 0.4444 - val_loss: 1.5087 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 1.4895 - acc: 0.5556 - val_loss: 1.4851 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 1.4700 - acc: 0.5185 - val_loss: 1.4700 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 1.4476 - acc: 0.5556 - val_loss: 1.4470 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 1.4274 - acc: 0.5370 - val_loss: 1.4300 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 1.4052 - acc: 0.5370 - val_loss: 1.4108 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 1.3853 - acc: 0.6111 - val_loss: 1.3857 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 1.3601 - acc: 0.5926 - val_loss: 1.3661 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 1.3410 - acc: 0.5926 - val_loss: 1.3527 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 1.3146 - acc: 0.6296 - val_loss: 1.3282 - val_acc: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f834fd6d0>"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.fit(X_train ,np.array(y_train) , epochs = 10 , validation_data=(X_test, np.array(y_test)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operating-movement",
   "metadata": {},
   "source": [
    "the non encoded data sets the correct value relative to the categorial one\n",
    "\n",
    "training the whole dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "foster-science",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 2/20\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 3/20\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 4/20\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 5/20\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0010 - acc: 1.0000\n",
      "Epoch 6/20\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 9.6382e-04 - acc: 1.0000\n",
      "Epoch 7/20\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 9.6169e-04 - acc: 1.0000\n",
      "Epoch 8/20\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 9.6860e-04 - acc: 1.0000\n",
      "Epoch 9/20\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 7.9154e-04 - acc: 1.0000\n",
      "Epoch 10/20\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 7.3864e-04 - acc: 1.0000\n",
      "Epoch 11/20\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 6.9623e-04 - acc: 1.0000\n",
      "Epoch 12/20\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 7.0740e-04 - acc: 1.0000\n",
      "Epoch 13/20\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 6.8149e-04 - acc: 1.0000\n",
      "Epoch 14/20\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 6.5770e-04 - acc: 1.0000\n",
      "Epoch 15/20\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 6.0606e-04 - acc: 1.0000\n",
      "Epoch 16/20\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 6.0799e-04 - acc: 1.0000\n",
      "Epoch 17/20\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 5.3658e-04 - acc: 1.0000\n",
      "Epoch 18/20\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 4.7148e-04 - acc: 1.0000\n",
      "Epoch 19/20\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 4.5145e-04 - acc: 1.0000\n",
      "Epoch 20/20\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 4.4017e-04 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f89e94550>"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.fit(input_sequences_X , np.array(pred_y) , epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "structural-farming",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.save(\"friday_1.0.h5\") #saving the model ...\n",
    "\n",
    "# new_model = tf.keras.models.load_model('friday_1.0.h5')  loading .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "lined-devil",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing ..\n",
    "def process(x):\n",
    "  token_list = tokenizer.texts_to_sequences([x])\n",
    "  input_sequences = np.array(pad_sequences(token_list , maxlen=max_sequence_len, padding='pre'))\n",
    "  input_sequences = np.array([input_sequences[0]])\n",
    "  return input_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "available-guest",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = \"send email \"\n",
    "x1 = process(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "answering-rwanda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, numpy.ndarray)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x1) , type(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "spanish-industry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.7253993e-05 1.1728472e-05 9.9988902e-01 3.5651642e-06 6.8472895e-05]]\n"
     ]
    }
   ],
   "source": [
    "pred = model1.predict(X_train[0:1])\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "linear-paraguay",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "union-sally",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.0785245e-05 2.6128177e-05 9.9979657e-01 9.8109513e-06 9.6809468e-05]]\n"
     ]
    }
   ],
   "source": [
    "pred = model1.predict(x1)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "cheap-bundle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "foreign-reduction",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = tf.keras.models.load_model('friday_1.0.h5') #load our model .... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "controlled-liabilities",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.0785245e-05, 2.6128177e-05, 9.9979657e-01, 9.8109513e-06,\n",
       "        9.6809468e-05]], dtype=float32)"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.predict(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "colonial-entry",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = {0 : \"google\" , 1 : \"youtube\" , 2 : \"email\" , 3 : \"random\" , 4 : \"text\"  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "capable-supervisor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello...yoo\n",
      "[[0.05053078 0.03792108 0.022188   0.8690008  0.0203593 ]]\n",
      "random\n",
      "hello...open google\n",
      "[[9.9968040e-01 1.2614518e-04 4.5880617e-05 9.2123853e-05 5.5433444e-05]]\n",
      "google\n",
      "hello...search something in google\n",
      "[[9.9971956e-01 1.4216334e-04 1.9400621e-05 8.5732885e-05 3.3102373e-05]]\n",
      "google\n",
      "hello...open youtube\n",
      "[[7.9549129e-05 9.9982053e-01 2.6630301e-05 6.5576700e-05 7.9119945e-06]]\n",
      "youtube\n",
      "hello...i want to want video in youtube\n",
      "[[1.0775707e-03 9.7385186e-01 2.4818566e-02 8.7187516e-05 1.6490574e-04]]\n",
      "youtube\n",
      "hello...kdkdkkadkamcadcmdac\n",
      "[[0.05053078 0.03792108 0.022188   0.8690008  0.0203593 ]]\n",
      "random\n",
      "hello...send a message to elon\n",
      "[[1.9152676e-04 6.5778775e-05 3.1249521e-03 6.3315569e-04 9.9598455e-01]]\n",
      "text\n",
      "hello...send a email to mark\n",
      "[[6.0047580e-05 2.6037091e-05 9.9978906e-01 5.0126741e-06 1.1995454e-04]]\n",
      "email\n",
      "hello...send a email to no sorry send a text message to john\n",
      "[[1.2039385e-04 4.2079468e-05 5.5329071e-04 7.7940407e-04 9.9850488e-01]]\n",
      "text\n",
      "hello...wowowoow\n",
      "[[0.05053078 0.03792108 0.022188   0.8690008  0.0203593 ]]\n",
      "random\n",
      "hello...send a text no sorry send a email to me  \n",
      "[[1.0319372e-04 4.4419652e-05 9.9965203e-01 8.5162510e-06 1.9191513e-04]]\n",
      "email\n",
      "hello...c\n"
     ]
    }
   ],
   "source": [
    "bot = True \n",
    "while bot:\n",
    "    text = input(\"hello...\")\n",
    "    if(text == \"c\"):\n",
    "        break \n",
    "    text = process(text)\n",
    "    pred = new_model.predict(text)\n",
    "    print(pred)\n",
    "    pred = np.argmax(pred)\n",
    "    print(predict[pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accredited-short",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driven-peeing",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
